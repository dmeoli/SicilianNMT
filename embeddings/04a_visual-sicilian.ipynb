{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Visual Sicilian\n",
    "\n",
    "###  Eryk Wdowiak\n",
    "\n",
    "This notebook attempts to illustrate the Sicilian text that we're using to develop a neural machine translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import *\n",
    "\n",
    "# import string\n",
    "# import re\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import nd\n",
    "import gluonnlp as nlp\n",
    "\n",
    "from data import transform_data_word2vec, preprocess_dataset\n",
    "from model import SG, CBOW\n",
    "from utils import print_time\n",
    "\n",
    "context = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  on the Sicilian side, this function only runs NLTK tokenizer\n",
    "##  on the English side, it will do much more\n",
    "def process_line(line):\n",
    "    tokens = word_tokenize(line)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  read in the lemmatized data\n",
    "df = pd.read_csv('dataset/train-mparamu_v2-lemmatized.sc', header=None)\n",
    "df.columns = ['sc_text']\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  flatten data to count words\n",
    "proc_scn = list(map(process_line, df.sc_text))\n",
    "flat_scn = [item for sublist in proc_scn for item in sublist]\n",
    "freq_scn = FreqDist(flat_scn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_scn.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create counts\n",
    "scn_bar_words = [x[0] for x in freq_scn.most_common(25)]\n",
    "scn_bar_counts = [x[1] for x in freq_scn.most_common(25)]\n",
    "\n",
    "# put data into dictionary\n",
    "scn_dict = dict(zip(scn_bar_words, scn_bar_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the color of our bar graphs\n",
    "color = cm.viridis_r(np.linspace(.4, .8, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "axs.bar(scn_bar_words, scn_bar_counts, color=color)\n",
    "axs.title.set_text('most common Sicilian lemmas')\n",
    "\n",
    "for ax in fig.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "plt.savefig('wb-sc_lemmas.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cloud of Sicilian words by frequency\n",
    "wordcloud = WordCloud(colormap='Spectral').generate_from_frequencies(scn_dict)\n",
    "\n",
    "plt.figure(figsize=(10, 10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "plt.savefig('wb-sc_lemma-cloud.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make wordcloud from embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  load datafile (so that we can retrieve vocabulary)\n",
    "datafile = 'dataset/train-mparamu_v3-lemmatized.sc.tsv'\n",
    "\n",
    "##  CBOW model\n",
    "# model = CBOW\n",
    "# parmfile = './logs/sc-cbow-r4-e01.params'\n",
    "# fname_insert = 'cbow'\n",
    "\n",
    "##  skipgram model\n",
    "model = SG\n",
    "parmfile = './logs/sc-skip-r2-e23.params'\n",
    "fname_insert = 'skip'\n",
    "\n",
    "##  both trained with hyperparameters:\n",
    "output_dim = 300\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  load the data\n",
    "data = nlp.data.TSVDataset(datafile)\n",
    "data, vocab, idx_to_counts = preprocess_dataset(data)\n",
    "\n",
    "##  load the model\n",
    "embedding = model(token_to_idx=vocab.token_to_idx, output_dim=output_dim,\n",
    "                  batch_size=batch_size,  #num_negatives=num_negatives,\n",
    "                  negatives_weights=mx.nd.array(idx_to_counts))\n",
    "embedding.load_parameters(parmfile)\n",
    "\n",
    "##  get the word vectors\n",
    "wvecs = embedding.embedding_out.weight.data()\n",
    "\n",
    "##  count words with at least \"min_words\" appearances\n",
    "min_words = 10\n",
    "num_over_min = len(\n",
    "    np.array(idx_to_counts)[np.array(idx_to_counts) >= min_words])\n",
    "\n",
    "print('vocabulary length:    ' + str(len(vocab)))\n",
    "print('lemmas over ' + str(min_words) + ' times: ' + str(num_over_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  pairwise cosine similarity\n",
    "def cos_sim(wordx, wordy):\n",
    "    xx = wvecs[vocab.token_to_idx[wordx],]\n",
    "    yy = wvecs[vocab.token_to_idx[wordy],]\n",
    "    return nd.dot(xx, yy) / (nd.norm(xx) * nd.norm(yy))\n",
    "\n",
    "\n",
    "##  full matrix of cosine similarity\n",
    "def cos_mat(vecs):\n",
    "    ##  dot product divided by the norms\n",
    "    xtx = nd.dot(vecs, vecs.T)\n",
    "    nmx = nd.sqrt(nd.diag(xtx)).reshape((-1, 1))\n",
    "    cnm = nd.dot(nmx, nmx.T)\n",
    "    return xtx / cnm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  create \"WC Dict\" (\"word-to-cosine dictionary\") for wordcloud\n",
    "def mk_wcdict(word, k_words):\n",
    "    ##  where to start?  first two tokens are: <BOS> <EOS>\n",
    "    sv_start = 2\n",
    "\n",
    "    ##  get cosine matrix\n",
    "    cosmat = cos_mat(wvecs[sv_start:-1, ])\n",
    "\n",
    "    ##  get the row of cosines\n",
    "    idx_to_lookup = vocab.token_to_idx[word] - sv_start\n",
    "    row_looked_up = cosmat[idx_to_lookup,]\n",
    "\n",
    "    ##  nearest neighbors by cosine similarity\n",
    "    knn_cosmat = row_looked_up.argsort()[::-1][1:k_words + 1].astype(\n",
    "        int).asnumpy()\n",
    "\n",
    "    ##  indexes of nearest neighbors in vocab list\n",
    "    knn_vocab_idx = list(knn_cosmat + sv_start)\n",
    "\n",
    "    ##  get the words and cosine measures\n",
    "    knn_vocab_words = [vocab.idx_to_token[idx] for idx in knn_vocab_idx]\n",
    "    knn_vocab_cosines = [cosmat[idx_to_lookup, idx].asnumpy()[0] for idx in\n",
    "                         knn_cosmat]\n",
    "\n",
    "    ##  return the dictionary for wordcloud\n",
    "    return dict(zip(knn_vocab_words, knn_vocab_cosines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cloud of 25 words for Don Chisciotti!\n",
    "knn_wc_dict = mk_wcdict('chisciotti', 25)\n",
    "wordcloud = WordCloud(colormap='Spectral').generate_from_frequencies(\n",
    "    knn_wc_dict)\n",
    "\n",
    "plt.figure(figsize=(10, 10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "fname = 'wc-sc-' + fname_insert + '_chisciotti.png'\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cloud of 25 words for Sanciu Panza!\n",
    "knn_wc_dict = mk_wcdict('sanciu', 25)\n",
    "wordcloud = WordCloud(colormap='Spectral').generate_from_frequencies(\n",
    "    knn_wc_dict)\n",
    "\n",
    "plt.figure(figsize=(10, 10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "fname = 'wc-sc-' + fname_insert + '_sanciu.png'\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scn_bi_finder = BigramCollocationFinder.from_words(flat_scn)\n",
    "# scn_bi_finder.apply_freq_filter(5)\n",
    "scn_bi_scored = scn_bi_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "scn_bi_scored[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scn_bi_pmi_finder = BigramCollocationFinder.from_words(flat_scn)\n",
    "scn_bi_pmi_finder.apply_freq_filter(5)\n",
    "scn_bi_pmi_scored = scn_bi_pmi_finder.score_ngrams(bigram_measures.pmi)\n",
    "scn_bi_pmi_scored[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scn_tri_finder = TrigramCollocationFinder.from_words(flat_scn)\n",
    "# scn_tri_finder.apply_freq_filter(5)\n",
    "scn_tri_scored = scn_tri_finder.score_ngrams(trigram_measures.raw_freq)\n",
    "scn_tri_scored[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scn_tri_pmi_finder = TrigramCollocationFinder.from_words(flat_scn)\n",
    "scn_tri_pmi_finder.apply_freq_filter(5)\n",
    "scn_tri_pmi_scored = scn_tri_pmi_finder.score_ngrams(trigram_measures.pmi)\n",
    "scn_tri_pmi_scored[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
