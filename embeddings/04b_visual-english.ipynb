{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Visual English\n",
    "\n",
    "###  Eryk Wdowiak\n",
    "\n",
    "This notebook attempts to illustrate the English text that we're using to develop a neural machine translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import *\n",
    "\n",
    "# import string\n",
    "# import re\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import nd\n",
    "import gluonnlp as nlp\n",
    "\n",
    "from data import transform_data_word2vec, preprocess_dataset\n",
    "from model import SG, CBOW\n",
    "from utils import print_time\n",
    "\n",
    "context = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  I thought this function would do far more than just run NLTK tokenizer.\n",
    "##  We'll leave it in place.  It keeps our options open.\n",
    "def process_line(line):\n",
    "    tokens = word_tokenize(line)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  read in the lemmatized data\n",
    "df = pd.read_csv('dataset/train-mparamu_v2-lemmatized.en',header=None)\n",
    "df.columns = ['en_text']\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  flatten data to count words\n",
    "proc_eng = list(map(process_line, df.en_text))\n",
    "flat_eng = [item for sublist in proc_eng for item in sublist]\n",
    "freq_eng = FreqDist(flat_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_eng.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create counts\n",
    "eng_bar_words = [x[0] for x in freq_eng.most_common(25)]\n",
    "eng_bar_counts = [x[1] for x in freq_eng.most_common(25)]\n",
    "\n",
    "# put data into dictionary\n",
    "eng_dict = dict(zip(eng_bar_words, eng_bar_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the color of our bar graphs\n",
    "color = cm.viridis_r(np.linspace(.4,.8, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(8,4))\n",
    "\n",
    "axs.bar(eng_bar_words, eng_bar_counts , color=color)\n",
    "axs.title.set_text('most common English lemmas')\n",
    "\n",
    "for ax in fig.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "plt.savefig('wb-en_lemmas.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cloud of Sicilian words by frequency\n",
    "wordcloud = WordCloud(colormap='Spectral').generate_from_frequencies(eng_dict)\n",
    "\n",
    "plt.figure(figsize=(10,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "plt.savefig('wb-en_lemma-cloud.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make wordcloud from embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  load datafile (so that we can retrieve vocabulary)\n",
    "datafile = 'dataset/train-mparamu_v3-lemmatized.en.tsv'\n",
    "\n",
    "##  CBOW model\n",
    "model = CBOW\n",
    "parmfile = './logs/en-cbow-r4-e01.params'\n",
    "fname_insert = 'cbow'\n",
    "\n",
    "##  skipgram model\n",
    "# model = SG\n",
    "# parmfile = './logs/en-skip-r2-e24.params'\n",
    "# fname_insert = 'skip'\n",
    "\n",
    "##  both trained with hyperparameters:\n",
    "output_dim = 300\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  load the data\n",
    "data = nlp.data.TSVDataset(datafile)\n",
    "data, vocab, idx_to_counts = preprocess_dataset( data ) \n",
    "\n",
    "##  load the model\n",
    "embedding = model(token_to_idx=vocab.token_to_idx, output_dim=output_dim,\n",
    "                  batch_size=batch_size, #num_negatives=num_negatives,\n",
    "                  negatives_weights=mx.nd.array(idx_to_counts))\n",
    "embedding.load_parameters(parmfile)\n",
    "\n",
    "##  get the word vectors\n",
    "wvecs = embedding.embedding_out.weight.data()\n",
    "\n",
    "##  count words with at least \"min_words\" appearances\n",
    "min_words = 10\n",
    "num_over_min = len( np.array(idx_to_counts)[ np.array(idx_to_counts)>= min_words ] )\n",
    "\n",
    "print('vocabulary length:    ' + str(len(vocab)))\n",
    "print('lemmas over ' + str(min_words) + ' times: ' + str(num_over_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  pairwise cosine similarity\n",
    "def cos_sim(wordx, wordy):\n",
    "    xx = wvecs[vocab.token_to_idx[wordx],]\n",
    "    yy = wvecs[vocab.token_to_idx[wordy],]\n",
    "    return nd.dot(xx, yy) / (nd.norm(xx) * nd.norm(yy))\n",
    "\n",
    "##  full matrix of cosine similarity\n",
    "def cos_mat( vecs ):\n",
    "    ##  dot product divided by the norms\n",
    "    xtx = nd.dot( vecs , vecs.T)\n",
    "    nmx = nd.sqrt( nd.diag(xtx) ).reshape((-1,1))\n",
    "    cnm = nd.dot( nmx , nmx.T )\n",
    "    return xtx / cnm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  create \"WC Dict\" (\"word-to-cosine dictionary\") for wordcloud\n",
    "def mk_wcdict(word,k_words):\n",
    "    \n",
    "    ##  where to start?  first two tokens are: <BOS> <EOS>\n",
    "    sv_start = 2\n",
    "    \n",
    "    ##  get cosine matrix\n",
    "    cosmat = cos_mat( wvecs[sv_start:-1,] )\n",
    "    \n",
    "    ##  get the row of cosines\n",
    "    idx_to_lookup = vocab.token_to_idx[word] - sv_start\n",
    "    row_looked_up = cosmat[idx_to_lookup,]\n",
    "    \n",
    "    ##  nearest neighbors by cosine similarity\n",
    "    knn_cosmat = row_looked_up.argsort()[::-1][1:k_words+1].astype(int).asnumpy()\n",
    "    \n",
    "    ##  indexes of nearest neighbors in vocab list\n",
    "    knn_vocab_idx = list(knn_cosmat + sv_start)\n",
    "    \n",
    "    ##  get the words and cosine measures\n",
    "    knn_vocab_words = [vocab.idx_to_token[idx] for idx in knn_vocab_idx]\n",
    "    knn_vocab_cosines = [cosmat[idx_to_lookup,idx].asnumpy()[0] for idx in knn_cosmat]\n",
    "        \n",
    "    ##  return the dictionary for wordcloud\n",
    "    return dict(zip(knn_vocab_words,knn_vocab_cosines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cloud of 25 words for Don Chisciotti!\n",
    "knn_wc_dict = mk_wcdict('chisciotti',25)\n",
    "wordcloud = WordCloud(colormap='Spectral').generate_from_frequencies(knn_wc_dict)\n",
    "\n",
    "plt.figure(figsize=(10,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "fname = 'wc-en-' + fname_insert + '_chisciotti.png'\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cloud of 25 words for Sanciu Panza!\n",
    "knn_wc_dict = mk_wcdict('sanciu',25)\n",
    "wordcloud = WordCloud(colormap='Spectral').generate_from_frequencies(knn_wc_dict)\n",
    "\n",
    "plt.figure(figsize=(10,10), facecolor='k')\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "fname = 'wc-en-' + fname_insert + '_sanciu.png'\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_bi_finder = BigramCollocationFinder.from_words(flat_eng)\n",
    "# eng_bi_finder.apply_freq_filter(5)\n",
    "eng_bi_scored = eng_bi_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "eng_bi_scored[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_bi_pmi_finder = BigramCollocationFinder.from_words(flat_eng)\n",
    "eng_bi_pmi_finder.apply_freq_filter(5)\n",
    "eng_bi_pmi_scored = eng_bi_pmi_finder.score_ngrams(bigram_measures.pmi)\n",
    "eng_bi_pmi_scored[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tri_finder = TrigramCollocationFinder.from_words(flat_eng)\n",
    "# eng_tri_finder.apply_freq_filter(5)\n",
    "eng_tri_scored = eng_tri_finder.score_ngrams(trigram_measures.raw_freq)\n",
    "eng_tri_scored[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tri_pmi_finder = TrigramCollocationFinder.from_words(flat_eng)\n",
    "eng_tri_pmi_finder.apply_freq_filter(5)\n",
    "eng_tri_pmi_scored = eng_tri_pmi_finder.score_ngrams(trigram_measures.pmi)\n",
    "eng_tri_pmi_scored[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
